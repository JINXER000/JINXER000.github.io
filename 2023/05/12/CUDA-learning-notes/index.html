<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Yizhou (Joseph) Chen</title><meta name="author" content="Yizhou (Joseph) Chen"><link rel="shortcut icon" href="https://upload.wikimedia.org/wikipedia/commons/5/50/Emblem_of_CU.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Yizhou (Joseph) Chen</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/"> About</a></li><li class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/yizhou-black.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Yizhou (Joseph) Chen</h3><p class="author-bio">An engineer and researcher in robotics.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://github.com/JINXER000" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="http://www.mae.cuhk.edu.hk/~usr/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://www.linkedin.com/in/%E5%A5%95%E5%B7%9E-%E9%99%88-49710a154/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://outlook.office.com/mail/deeplink/compose?mailtouri=mailto%3Ajosephchen%40link.cuhk.edu.hk" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/yzchen-CVphd.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">CUDA learning notes</h2><article><p>@[toc]</p>
<h1 id="cuda-tutorial"><a href="#cuda-tutorial" class="headerlink" title="cuda tutorial"></a>cuda tutorial</h1><p>这是我个人学习过程中的一些笔记，菜鸟一枚。<br>抱歉所有的图都挂了，有空的时候慢慢恢复。<br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/">cuda programming guide</a><br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/">NVIDIA best practice guide</a><br><a target="_blank" rel="noopener" href="https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670611900923">CS344 udacity website</a></p>
<ul>
<li>got an overview on the usage of stereo camera based on Yingcai’s code</li>
</ul>
<h2 id="communication-pattern"><a href="#communication-pattern" class="headerlink" title="communication pattern"></a>communication pattern</h2><ul>
<li>map<br>一对一，每个pix执行同样的函数，比如×2</li>
<li>gather<br>多对一，比如图像模糊<br><img src="https://img-blog.csdnimg.cn/20200321120351574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>scatter<br>同一个线程尝试写到许多memory，一对多，可能冲突。<br><img src="https://img-blog.csdnimg.cn/20200321120453404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>stencil<br>好像是前两个的结合，在临近点gather。</li>
<li>transpose</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://postimg.cc/dLNWSTYV"><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pLnBvc3RpbWcuY2MvTDgycjV6UUwvaW1hZ2UucG5n?x-oss-process=image/format,png" alt="image.png"></a></p>
<h2 id="memery-model"><a href="#memery-model" class="headerlink" title="memery model"></a>memery model</h2><p>using global memery is much slower than using local and shared memeries.</p>
<ul>
<li>varibles defined in kernel func are local varibles.</li>
<li>use <strong>shared</strong> memery:<br><img src="https://img-blog.csdnimg.cn/20200322171257385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-QsYq76Sy-1583324831676)(https://i.postimg.cc/3rVP2k9f/image.png)]"></li>
</ul>
<p>put frequently used memory into shared memory!</p>
<h2 id="syncronize"><a href="#syncronize" class="headerlink" title="syncronize"></a>syncronize</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">int</span> arry[<span class="number">128</span>];</span><br><span class="line">array[idx]=threadIdx;</span><br><span class="line"><span class="comment">// wait for every read op to compelete</span></span><br><span class="line">__syncthreads();</span><br><span class="line"><span class="type">int</span> temp=array[idx+<span class="number">1</span>];</span><br><span class="line"><span class="comment">// write from shared mem to local mem</span></span><br><span class="line">__syncthreads();</span><br><span class="line">array[idx]=temp;</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure>

<h2 id="atomic-opp"><a href="#atomic-opp" class="headerlink" title="atomic opp"></a>atomic opp</h2><ul>
<li>is equivlent of adding variables in shared mem, but actually in global memery. </li>
<li>will slow down the app</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">g[i]=g[i]+1    ------&gt;  atomicAdd(&amp;g[i],1);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>realize push_back in kernel func.</li>
</ul>
<h2 id="thrust"><a href="#thrust" class="headerlink" title="thrust"></a>thrust</h2><ul>
<li>vector</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">thrust::device_vector&lt;<span class="type">int</span>&gt; dv&lt;<span class="number">100</span>&gt;;</span><br><span class="line">thrust::host_vector&lt;<span class="type">int</span>&gt; hv&lt;<span class="number">100</span>,<span class="number">25</span>&gt;;</span><br><span class="line"><span class="comment">// cudaMemcopy() in the background</span></span><br><span class="line">dv=hv;</span><br></pre></td></tr></table></figure>
<ul>
<li>Note: device_vector.push_back() cannot be used in kernel function <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/21786495/cuda-kernel-returning-vectors">example</a></li>
</ul>
<h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><ol>
<li>add 0.5 back elements with 0.5 front elements in per block</li>
<li>add 0.25 back elements with 0.25 front elements in per block</li>
<li>until 1 element in every block</li>
</ol>
<h2 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h2><ul>
<li>Applications of Scan</li>
</ul>
<ol>
<li>Stream Compaction</li>
<li>Summed-Area Tables :<br> A summed-area table (SAT) is a two-dimensional table generated from an input image in which each entry in the table stores the sum of all pixels between the entry location and the lower-left corner of the input image. often used when doing a  box filter on image.</li>
</ol>
<h3 id="serialize-implementation"><a href="#serialize-implementation" class="headerlink" title="serialize implementation"></a>serialize implementation</h3><ul>
<li>inclusive</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)&#123;</span><br><span class="line">	acc = acc + elements[i];</span><br><span class="line">	out[i] = acc;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>exclusive</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; ARRAY_SIZE; i++)&#123;</span><br><span class="line">    out[i] = acc;</span><br><span class="line">	acc = acc + elements[i];</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="parallel-implementation"><a href="#parallel-implementation" class="headerlink" title="parallel implementation"></a>parallel implementation</h3><h4 id="Hillis-and-Steele"><a href="#Hillis-and-Steele" class="headerlink" title="Hillis and Steele"></a>Hillis and Steele</h4><p><img src="https://img-blog.csdnimg.cn/20200322172443271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ul>
<li>In practice , it should be excuted in the same block which has more threads than the number of array.  </li>
<li>use double buffer, or the results of one warp will be overwritten by threads in another warp.</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">   <span class="function">__global__ <span class="type">void</span> <span class="title">scan</span><span class="params">(<span class="type">float</span> *g_odata, <span class="type">float</span> *g_idata, <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="type">float</span> temp[]; <span class="comment">// allocated on invocation</span></span><br><span class="line">   <span class="type">int</span> thid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> pout = <span class="number">0</span>, pin = <span class="number">1</span>;</span><br><span class="line">  <span class="comment">// Load input into shared memory.</span></span><br><span class="line">   <span class="comment">// This is exclusive scan, so shift right by one</span></span><br><span class="line">   <span class="comment">// and set first element to 0</span></span><br><span class="line">  temp[pout*n + thid] = (thid &gt; <span class="number">0</span>) ? g_idata[thid<span class="number">-1</span>] : <span class="number">0</span>;</span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> offset = <span class="number">1</span>; offset &lt; n; offset *= <span class="number">2</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    pout = <span class="number">1</span> - pout; <span class="comment">// swap double buffer indices</span></span><br><span class="line">    pin = <span class="number">1</span> - pout;</span><br><span class="line">    <span class="keyword">if</span> (thid &gt;= offset)</span><br><span class="line">      temp[pout*n+thid] += temp[pin*n+thid - offset];</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      temp[pout*n+thid] = temp[pin*n+thid];</span><br><span class="line">    __syncthreads();</span><br><span class="line">  &#125;</span><br><span class="line">  g_odata[thid] = temp[pout*n+thid]; <span class="comment">// write output</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>in&#x2F;out: 2 buffers</li>
<li>d: step</li>
<li>offset&#x3D;2^d</li>
</ul>
<h4 id="Blelloch"><a href="#Blelloch" class="headerlink" title="Blelloch"></a>Blelloch</h4><ul>
<li>To do this we will use an algorithmic pattern that arises often in parallel computing: balanced trees. The idea is to build a balanced binary tree on the input data and sweep it to and from the root to compute the prefix sum. A binary tree with n leaves has d &#x3D; log2 n levels, and each level d has 2 d nodes. If we perform one add per node, then we will perform O(n) adds on a single traversal of the tree.</li>
<li>The algorithm consists of two phases: the reduce phase (also known as the up-sweep phase) and the down-sweep phase.</li>
</ul>
<h5 id="upsweep"><a href="#upsweep" class="headerlink" title="upsweep"></a>upsweep</h5><p><img src="https://img-blog.csdnimg.cn/20200321182409432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Bscan</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span> *g_odata, <span class="type">int</span> *g_idata, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="type">int</span> temp[]; <span class="comment">// allocated on invocation</span></span><br><span class="line">  <span class="type">int</span> thid = threadIdx.x;</span><br><span class="line">  <span class="type">int</span> offset = <span class="number">1</span>;</span><br><span class="line">  temp[<span class="number">2</span> * thid] = g_idata[<span class="number">2</span> * thid]; <span class="comment">// load input into shared memory</span></span><br><span class="line">  temp[<span class="number">2</span> * thid + <span class="number">1</span>] = g_idata[<span class="number">2</span> * thid + <span class="number">1</span>];</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> d = n &gt;&gt; <span class="number">1</span>; d &gt; <span class="number">0</span>; d &gt;&gt;= <span class="number">1</span>) <span class="comment">// build sum in place up the tree</span></span><br><span class="line">  &#123;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="keyword">if</span> (thid &lt; d) &#123;</span><br><span class="line">      <span class="type">int</span> ai = offset * (<span class="number">2</span> * thid + <span class="number">1</span>) - <span class="number">1</span>;</span><br><span class="line">      <span class="type">int</span> bi = offset * (<span class="number">2</span> * thid + <span class="number">2</span>) - <span class="number">1</span>;</span><br><span class="line">      temp[bi] += temp[ai];</span><br><span class="line">    &#125;</span><br><span class="line">    offset *= <span class="number">2</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (thid == <span class="number">0</span>) &#123;</span><br><span class="line">    temp[n - <span class="number">1</span>] = <span class="number">0</span>;</span><br><span class="line">  &#125;                              <span class="comment">// clear the last element</span></span><br></pre></td></tr></table></figure>
<ul>
<li>size of temp: 2^d+2^(d-1)+….1&#x3D;2^(d+1)&#x3D;2n</li>
<li>store all intermedia values(binary tree)</li>
</ul>
<h5 id="down-sweep"><a href="#down-sweep" class="headerlink" title="down sweep"></a>down sweep</h5><p><a target="_blank" rel="noopener" href="https://postimg.cc/5HPYYVNT">[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-LMt9zDhw-1583324721325)(https://i.postimg.cc/cHWQz1qx/image.png)]</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">1</span>; d &lt; n; d *= <span class="number">2</span>) <span class="comment">// traverse down tree &amp; build scan</span></span><br><span class="line">&#123;</span><br><span class="line">  offset &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="keyword">if</span> (thid &lt; d) &#123;</span><br><span class="line">    <span class="type">int</span> ai = offset * (<span class="number">2</span> * thid + <span class="number">1</span>) - <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> bi = offset * (<span class="number">2</span> * thid + <span class="number">2</span>) - <span class="number">1</span>;</span><br><span class="line">    <span class="type">float</span> t = temp[ai];</span><br><span class="line">    temp[ai] = temp[bi];</span><br><span class="line">    temp[bi] += t;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">__syncthreads();</span><br><span class="line">g_odata[<span class="number">2</span> * thid] = temp[<span class="number">2</span> * thid]; <span class="comment">// write results to device memory</span></span><br><span class="line">g_odata[<span class="number">2</span> * thid + <span class="number">1</span>] = temp[<span class="number">2</span> * thid + <span class="number">1</span>];</span><br></pre></td></tr></table></figure>

<ul>
<li><p>which scan to choose?<br>if limited processor: we should choose the one that require  less work num, i.e., Work Efficient （balloc）<br>else: Step Efficient   (Hill)<br><a target="_blank" rel="noopener" href="https://postimg.cc/8JwjYH7T">[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-qDhgX6Ov-1583324721325)(https://i.postimg.cc/NFvm5CY2/image.png)]</a></p>
</li>
<li><p>refer from:<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html">nvidia tutorial</a></p>
</li>
</ul>
<h3 id="segment-scan"><a href="#segment-scan" class="headerlink" title="segment scan"></a>segment scan</h3><p>一个长array分成许多小array，然后在里面scan.<br>用在稀疏矩阵乘法，稠密向量乘法。<br>page rank: n*n,有链接的才不是0<br>这时，就可以使用segment scan将稀疏矩阵表示成CSR format，<br><img src="https://img-blog.csdnimg.cn/20200321115535502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h2><h3 id="odd-even-sort"><a href="#odd-even-sort" class="headerlink" title="odd-even sort"></a>odd-even sort</h3><p>parallel version of bubble sort.</p>
<ul>
<li>work: O(n^2)</li>
<li>step: O(n)</li>
</ul>
<h3 id="merge-sort"><a href="#merge-sort" class="headerlink" title="merge sort"></a>merge sort</h3><p>归并排序, 3 main stages<br>Stage 2 use 1 thread-block.<br>Note that in stage 3, only 1 thread works and lots of SM will be idle. So we break 2 list into sub-lists to achieve parallesm.</p>
<ul>
<li>work: O(n*logn)</li>
<li>step: O(logn)<br><img src="https://img-blog.csdnimg.cn/20200305213109592.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<h3 id="sorting-networks"><a href="#sorting-networks" class="headerlink" title="sorting networks"></a>sorting networks</h3><p>bitonic sort双调排序，计算量oblivious to input content. 无论是random, sorted , reverted array，都一样。<br><img src="https://img-blog.csdnimg.cn/20200305213748347.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="radix-sort"><a href="#radix-sort" class="headerlink" title="radix sort"></a>radix sort</h2><p>从LSB（最小端）开始，把0放到前面，1放到后面，一直到最高位比较完。<br>每一位的比较，其实用的是compact, 也就是scan。<br><img src="https://img-blog.csdnimg.cn/20200305224048137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="quick-sort"><a href="#quick-sort" class="headerlink" title="quick sort"></a>quick sort</h3><p>本质是一个递归算法，先取pivot，分成&lt;,&#x3D;,&gt;三个array,然后在三个array中继续取pivot。以下是非递归实现。可以用dynamic parallism来递归。<br><img src="https://img-blog.csdnimg.cn/20200305224936488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="optimization"><a href="#optimization" class="headerlink" title="optimization"></a>optimization</h2><p>几个决定速度的方面：</p>
<ol>
<li>算法，用算术复杂度来衡量</li>
<li>基本原则，cache-aware实现</li>
<li>基于平台架构的优化;</li>
<li>小优化，比如快速逆平方根<br><img src="https://img-blog.csdnimg.cn/20200322171757865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ol>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>首先分析自己代码是否利用了bandwidth。<br>使用device query分析，可以计算出GPU的带宽（时钟和bus），看看自己写的kernel是否完全利用了带宽。<br><img src="https://img-blog.csdnimg.cn/20200322211907830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这很可能是coalesced的缘故.coalesce也就是，threadIdx.x相邻的线程应该访问相邻的元素，否则如果跨度大的话，memory transaction中很大一部分就浪费。、</p>
<ul>
<li>LITTLE‘S LAW<br><img src="https://img-blog.csdnimg.cn/20200322232344624.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200322232619396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>有sharemem时候，降低latency的方法：<br><img src="https://img-blog.csdnimg.cn/20200322233947169.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>第二个是因为，如果32<em>32个thread，可能会有很多线程等待其他线程，改成16</em>16就会好很多。<br>第四个是因为，一个SM有很多block，一个block在等sycthread()，其他的block可以行动。</li>
</ul>
<h3 id="thread-divergence"><a href="#thread-divergence" class="headerlink" title="thread divergence"></a>thread divergence</h3><p>一个warp中thread因为if\else而异步进行。慢了多少主要看一个warp被分成了多少份。<br><img src="https://img-blog.csdnimg.cn/20200323212821852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="一些ninja-method"><a href="#一些ninja-method" class="headerlink" title="一些ninja method"></a>一些ninja method</h3><ol>
<li>数字默认double，所以后面加上f会快，比如2.5f</li>
<li>使用intrinsic<br><img src="https://img-blog.csdnimg.cn/20200323222628829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ol>
<h4 id="device-query"><a href="#device-query" class="headerlink" title="device query"></a>device query</h4><p>5.14. 看看你硬件<br>.&#x2F;deviceQuery<br>1060: 10 SM* 192， 1024 threads&#x2F;1 block<br>2060: 30 SM* 64, 	1024 threads&#x2F;1 block<br>TX2: 2 SM*64, 1024<br>XAVIER NX: 6SM * 64, 1024<br><img src="https://img-blog.csdnimg.cn/2020041617473939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="pined-memory"><a href="#pined-memory" class="headerlink" title="pined memory"></a>pined memory</h4><p>使用cudaHostMalloc可以让CPU到GPU的copy更快。可以用于hash-table的streaming。<br><img src="https://img-blog.csdnimg.cn/20200323223041875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h4><p>让不同的kernel同时运行。两个kernel没有相互依赖关系才可以。<br><img src="https://img-blog.csdnimg.cn/20200323223359922.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>下图也是同时的，s1和s2互不相干;<br><img src="https://img-blog.csdnimg.cn/20200323224359989.png" alt="在这里插入图片描述"><br>注意不要出现这种冲突的情况<br><img src="https://img-blog.csdnimg.cn/20200323224718260.png" alt="在这里插入图片描述"><br>stream的主要作用在，如果有一大陀数据，没法在一个kernel里全跑完，那就一小块一小块的考，比如一半在copy一半在process.让data transfering和processing同时进行。<br><img src="https://img-blog.csdnimg.cn/20200323225106624.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="list-ranking"><a href="#list-ranking" class="headerlink" title="list ranking"></a>list ranking</h3><p>就是把一个linked list变成array，给每个元素标号。<br>用更多的work(n* logn)来换更少的step(log n)。<br>本质思想是从找linked list最后一个元素来的。 每一个elem都找最后一个元素，然后从0开始wake, 一层一层wake。比如我们先wake 5<br><img src="https://img-blog.csdnimg.cn/20200416003152754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200416003106341.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3 id="cuckoo-hashing"><a href="#cuckoo-hashing" class="headerlink" title="cuckoo hashing"></a>cuckoo hashing</h3><p>chaining is bad for parallel.<br>kicking out things that already in the hash table.<br><img src="https://img-blog.csdnimg.cn/20200306202104347.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>有一定几率，每一个hash function都试过以后放不进去任何hash table。一定iteration之后，只能更换hash function了。<br>在lookup的时候，可能要把每一个hash function都试一遍。</p>
<ul>
<li>注意：<br>write in和Kick out操作，需要atomic operation(AtomicExch)</li>
</ul>
<h3 id="dynamic-parallelism"><a href="#dynamic-parallelism" class="headerlink" title="dynamic parallelism"></a>dynamic parallelism</h3><p><img src="https://img-blog.csdnimg.cn/20200320114432688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>让递归和nested 成为可能！</p>
<h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h4><ol>
<li>block里面每一个thread都会launch一个child block，可以使用threadIdx.x来限制;</li>
<li>stream, event都只属于某个block，不能把他们pass到其他block或者子block。我还不懂，要看看lesson5。</li>
<li>shared memory也是private的，没法pass给 child block。child block在另一个grid里面！<br><img src="https://img-blog.csdnimg.cn/20200320120716540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>第一次知道kernel里面还能malloc()…</li>
</ol>
<ul>
<li>quicksort的痛点和bfs是一样的！</li>
</ul>
<ol>
<li>每次执行完一个kernel,都需要把gpu信息(output_len, is_change)传到cpu</li>
<li>wave形式，wave短的要等wave长的<br><img src="https://img-blog.csdnimg.cn/20200320122103973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>结合cuda stream:<br><img src="https://img-blog.csdnimg.cn/20200320142007807.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ol>
<h3 id="matrix-multiply"><a href="#matrix-multiply" class="headerlink" title="matrix multiply"></a>matrix multiply</h3><p>稀疏矩阵使用CSR格式。这里的x是一个列向量。<br><img src="https://img-blog.csdnimg.cn/20200407224335842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="cudaMallocPitch-and-cudaMemcpy2D"><a href="#cudaMallocPitch-and-cudaMemcpy2D" class="headerlink" title="cudaMallocPitch and cudaMemcpy2D"></a>cudaMallocPitch and cudaMemcpy2D</h2><p>When accessing 2D arrays in CUDA, memory transactions are much faster if each row is properly aligned..</p>
<p>Assuming that we want to allocate a 2D padded array of floating point (single precision) elements:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMallocPitch(&amp;devPtr, &amp;devPitch, Ncols * sizeof(float), Nrows);</span><br></pre></td></tr></table></figure>
<p>where</p>
<ul>
<li>devPtr is an output pointer to float (float *devPtr);</li>
<li>devPitch is a size_t output variable denoting the length, in bytes, of the padded row;</li>
<li>Nrows and Ncols are size_t input variables representing the matrix size.</li>
</ul>
<p>cudaMallocPitch will allocate a memory space of size, in bytes, equal to Nows * pitch. However, only the first Ncols * sizeof(float) bytes of each row will contain the matrix data.<br>Accordingly, cudaMallocPitch consumes more memory than strictly necessary for the 2D matrix storage, but this is returned in more efficient memory accesses.</p>
<p>CUDA provides also the cudaMemcpy2D function to copy data from&#x2F;to host memory space to&#x2F;from device memory space allocated with cudaMallocPitch.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy2D(devPtr, devPitch, hostPtr, hostPitch, Ncols * sizeof(float), Nrows, cudaMemcpyHostToDevice)</span><br></pre></td></tr></table></figure>

<p>where</p>
<ul>
<li>devPtr and hostPtr are input pointers to float (float *devPtr and float *hostPtr) pointing to the - (source) device and (destination) host memory spaces, respectively;</li>
<li>devPitch and hostPitch are size_t input variables denoting the length, in bytes, of the padded rows for the device and host memory spaces, respectively;</li>
<li>Nrows and Ncols are size_t input variables representing the matrix size.</li>
</ul>
<h3 id="other-operators"><a href="#other-operators" class="headerlink" title="other operators"></a>other operators</h3><h4 id="ldg"><a href="#ldg" class="headerlink" title="__ldg"></a>__ldg</h4><p>optimize by using read-only cache.</p>
<p><img src="https://img-blog.csdnimg.cn/20200304231126987.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>refer: <a target="_blank" rel="noopener" href="https://www.olcf.ornl.gov/wp-content/uploads/2013/02/GPU_Opt_Fund-CW1.pdf">cuda sheet</a></p>
<h4 id="cudaMemcopyAsyc"><a href="#cudaMemcopyAsyc" class="headerlink" title="cudaMemcopyAsyc"></a>cudaMemcopyAsyc</h4><p>make use of stream. Copy engine and kernel engine can work cuncurrently. </p>
<h4 id="cudaMallocManaged"><a href="#cudaMallocManaged" class="headerlink" title="cudaMallocManaged"></a>cudaMallocManaged</h4><p>data on host and device can share same pointer. May be slower than cudaMalloc.</p>
<h4 id="ballot-bfind"><a href="#ballot-bfind" class="headerlink" title="ballot, bfind"></a>ballot, bfind</h4><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/cuancuancuanhao/p/7841512.html">cuda programming guide翻译</a></p>
<ul>
<li>create a bit mask in a 32 bits register using the GPU <em>ballot</em> instruction.</li>
<li>use the <em>bfind</em> PTX intrinsic to get the location of the first nonzero bit</li>
</ul>
<h3 id="how-to-optimize"><a href="#how-to-optimize" class="headerlink" title="how to optimize"></a>how to optimize</h3><h4 id="warp"><a href="#warp" class="headerlink" title="warp"></a>warp</h4><p>32 thread forms a warp. do computation concurrently in physical.</p>
<ul>
<li>How multiple warp parallize?<br>use computation(green) to hide latency(white).<br><img src="https://img-blog.csdnimg.cn/20200305173142352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ul>
<h4 id="memory-access-pattern"><a href="#memory-access-pattern" class="headerlink" title="memory access pattern"></a>memory access pattern</h4><p><img src="https://img-blog.csdnimg.cn/20200304214228292.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://img-blog.csdnimg.cn/20200304214340470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>so voxel hashing is not a good access pattern. </p>
<ol>
<li>voxel is not in native word length</li>
<li>not aligned, not coalesced. (random.<br><img src="https://img-blog.csdnimg.cn/20200304220549327.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
</ol>
<h4 id="share-mem"><a href="#share-mem" class="headerlink" title="share mem"></a>share mem</h4><h5 id="bank-conflict"><a href="#bank-conflict" class="headerlink" title="bank conflict"></a>bank conflict</h5><p>thread根据横着的id来分warp，0–31是warp 1，32–65是warp2.<br>share mem根据2d的id来分bank，一共有32个bank，对应thread的一个warp。横着的id&lt;31时，id每次+1,那么bank就+1。这样设计是因为，同一个warp里的thread访问到同一个bank里不同地址，就会conflict。<br>万一conflict怎么办呢？<br>可以在sharemem最右边pad一个column，这个column纯粹是占位用的，不参加IO。这样同一个warp就可以错开来访问不同的bank。具体参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/endlch/article/details/47043069">共享内存csdn</a><br><img src="https://img-blog.csdnimg.cn/20200305111300388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJTlhFUg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2023 by Yizhou (Joseph) Chen</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>